# PyTorch Machine Learning Project Template

A **reusable, researchâ€‘grade PyTorch template repository** for training, evaluating, and reporting deep learning models.

This template was developed using **MNIST** as a reference task but is designed to scale seamlessly to **medical imaging, tabular data, and multimodal ML projects**.

---

## âœ… Key Features

- Clean, modular **project structure** (dataset / model / training / evaluation)
- Reproducible experiments (fixed seeds, saved configs)
- **tqdm** progress bars for training & validation
- Automatic **CSV logging** of metrics
- Autoâ€‘generated **training curves** (loss & accuracy)
- Qualitative **model predictions** on heldâ€‘out data
- Oneâ€‘click **PDF summary report** per experiment (ideal for research & manuscripts)
- Deviceâ€‘agnostic: CPU, CUDA, or Apple Silicon (MPS)

---

## ğŸ“ Repository Structure

```text
.
â”œâ”€â”€ data/                     # Datasets (autoâ€‘downloaded or mounted)
â”œâ”€â”€ runs/                     # All experiment outputs (autoâ€‘generated)
â”‚   â””â”€â”€ <runâ€‘id>/
â”‚       â”œâ”€â”€ checkpoints/       # Best model checkpoint
â”‚       â”œâ”€â”€ logs/             # CSV logs
â”‚       â”œâ”€â”€ reports/          # PNG figures + PDF summary
â”‚       â””â”€â”€ config.json       # Saved experiment configuration
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ datasets/             # Dataset & DataLoader definitions
â”‚   â”œâ”€â”€ models/               # PyTorch model architectures
â”‚   â”œâ”€â”€ training/             # Training & validation engines
â”‚   â”œâ”€â”€ evaluation/           # Inference & qualitative evaluation utilities
â”‚   â”œâ”€â”€ reports/              # PDF report builders
â”‚   â”œâ”€â”€ utils/                # Logging, metrics, seeding, plotting
â”‚   â””â”€â”€ config.py             # Central configuration
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_training.py       # Main entry point
â”œâ”€â”€ tests/                    # Unit & smoke tests
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§ª Example Use Case (MNIST Baseline)

MNIST is included as a **reference implementation** to verify pipeline correctness, reproducibility, and reporting infrastructure.

---

## ğŸ”§ Installation

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Running Training

```bash
python scripts/run_training.py
```

### Common options

```bash
python scripts/run_training.py   --epochs 10   --batch-size 64   --lr 1e-3   --num-workers 4   --samples 16
```

---

## ğŸ“Š Training Outputs

Each run automatically generates:

- `metrics.csv` â€“ epochâ€‘level metrics
- `training_curves.png` â€“ loss & accuracy plots
- `predictions_grid.png` â€“ qualitative predictions
- `summary_report.pdf` â€“ oneâ€‘page experiment summary

All outputs are saved under `runs/<run-id>/`.

---

## ğŸ”„ Reusing the Template

To adapt for a new ML project:

1. Add a new dataset loader in `src/datasets/`
2. Add or modify models in `src/models/`
3. Keep training, logging, and reporting unchanged

---